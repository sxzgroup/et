{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorcircuit as tc\n",
    "import numpy as np\n",
    "\n",
    "tc.set_dtype(\"complex128\")\n",
    "tc.set_contractor(\"cotengra\")\n",
    "K = tc.set_backend(\"tensorflow\")\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "# the code work for both cpu and gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D XXZ model\n",
    "\n",
    "Lx = 4\n",
    "Ly = 3\n",
    "lattice = tc.templates.graphs.Grid2DCoord(Lx, Ly).lattice_graph(pbc=True)\n",
    "h = tc.quantum.heisenberg_hamiltonian(lattice, hxx=1, hyy=1, hzz=0.8, hz=0.00000001)\n",
    "e0 = scipy.sparse.linalg.eigsh(K.numpy(h), k=1, which=\"SA\")[0]\n",
    "print(\"exact gs\", e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full Hamiltonian spectrum\n",
    "\n",
    "es, u = np.linalg.eigh(K.numpy(K.real(K.to_dense(h))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half-filling sector\n",
    "\n",
    "\n",
    "@K.jit\n",
    "def get_sumz(s):\n",
    "    c = tc.Circuit(Lx * Ly, inputs=s)\n",
    "    sumz = sum([c.expectation_ps(z=[j]) for j in range(Lx * Ly)])\n",
    "    return K.real(sumz)\n",
    "\n",
    "\n",
    "collect = []\n",
    "for i in range(2 ** (Lx * Ly)):\n",
    "    sumz = get_sumz(u[:, i])\n",
    "    if K.abs(sumz) < 0.1:\n",
    "        collect.append(i)\n",
    "len(collect)\n",
    "\n",
    "# overlap[collect] keeps only spectrum of the half-filling sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaginary-time evolved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EE for ITES\n",
    "\n",
    "\n",
    "@K.jit\n",
    "def get_ee(beta):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    ee = tc.quantum.entanglement_entropy(target, [0, 1, 4, 5, 8, 9])\n",
    "    return ee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(us):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    c = tc.Circuit(Lx * Ly, inputs=s)\n",
    "    energy = tc.templates.measurements.operator_expectation(c, h)\n",
    "    return K.real(energy)\n",
    "\n",
    "\n",
    "def get_state(us):\n",
    "    # Lx*Ly, chi, chi, 2\n",
    "    ns = []\n",
    "    for i in range(Lx * Ly):\n",
    "        ns.append(tc.gates.Gate(K.copy(us[i])))\n",
    "    for i in range(Lx * Ly):\n",
    "        ns[i][1] ^ ns[(i + 1) % (Lx * Ly)][0]\n",
    "    m = tc.contractor(ns, output_edge_order=[n[2] for n in ns])\n",
    "    s = K.reshape(m.tensor, [-1])\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_fidelity(us, gs):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    s = K.cast(s, tc.dtypestr)\n",
    "    return -K.abs(tf.tensordot(s, gs, 1)) ** 2\n",
    "\n",
    "\n",
    "vgf_fidelity = K.jit(K.value_and_grad(get_fidelity))\n",
    "vgf_e = K.jit(K.value_and_grad(get_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITES training\n",
    "\n",
    "\n",
    "def get_ites(beta, chi=16, steps=300):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    lst = tf.keras.optimizers.schedules.ExponentialDecay(3e-3, 1000, 0.5)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(lst))\n",
    "    hist = []\n",
    "    us = np.random.normal(scale=0.1, size=[Lx * Ly, chi, chi, 2])\n",
    "    for i in range(steps):\n",
    "        v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        hist.append(v)\n",
    "    state = get_state(us)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (v, slope, intercept, r, p, slopee, overlap0, overlap, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ites_bfgs(beta, chi=16, steps=300):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "\n",
    "    def loss_wrapper(vgf):\n",
    "        def f(*args, **kws):\n",
    "            v, gs = vgf(*args, **kws)\n",
    "            f.loss.append(v)\n",
    "            return v, gs\n",
    "\n",
    "        f.loss = []\n",
    "        return f\n",
    "\n",
    "    vgf_scipy = tc.interfaces.scipy_optimize_interface(\n",
    "        partial(get_fidelity, gs=target),\n",
    "        jit=True,\n",
    "        shape=[Lx * Ly, chi, chi, 2],\n",
    "        gradient=True,\n",
    "    )\n",
    "    us = np.random.normal(scale=0.1, size=[Lx * Ly, chi, chi, 2])\n",
    "    vgf_wrapper = loss_wrapper(vgf_scipy)\n",
    "    optr = scipy.optimize.minimize(\n",
    "        vgf_wrapper,\n",
    "        K.reshape(us, [-1]),\n",
    "        method=\"L-BFGS-B\",\n",
    "        jac=True,\n",
    "        options={\"ftol\": 1e-22, \"gtol\": 1e-22, \"maxiter\": steps},\n",
    "    )\n",
    "    us = K.reshape(optr.x, [Lx * Ly, chi, chi, 2])\n",
    "    state = get_state(us)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (optr, slope, intercept, r, p, slopee, overlap0, overlap, vgf_wrapper.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites(0.5, chi=32, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites_bfgs(1.5, chi=32, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training toward gs with history\n",
    "\n",
    "\n",
    "def get_gs_training(chi=16, steps=300, which=\"e\"):\n",
    "    target = u[:, 0]\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    lst = tf.keras.optimizers.schedules.ExponentialDecay(3e-3, 1000, 0.5)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(lst))\n",
    "    us = np.random.normal(scale=0.1, size=[Lx * Ly, chi, chi, 2])\n",
    "    hist = []\n",
    "    ovs = []\n",
    "    for i in range(steps):\n",
    "        if which == \"e\":\n",
    "            v, gs = vgf_e(us)\n",
    "        elif which == \"f\":\n",
    "            v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        if i % 5 == 0:\n",
    "            hist.append(v)\n",
    "            state = get_state(us)\n",
    "            state = state / K.norm(state)\n",
    "            overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "            ovs.append(overlap)\n",
    "    return hist, ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gs_training(chi=16, steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(us):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    c = tc.Circuit(Lx * Ly, inputs=s)\n",
    "    energy = tc.templates.measurements.operator_expectation(c, h)\n",
    "\n",
    "    return K.real(energy)\n",
    "\n",
    "\n",
    "def get_state(us):\n",
    "    # peps Lx, Ly, chil, chir, chiu, chid, 2\n",
    "    ns = {}\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            ns[(i, j)] = tc.gates.Gate(K.copy(us[i, j]))\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            ns[(i, j)][1] ^ ns[((i + 1) % Lx, j)][0]\n",
    "            ns[(i, j)][3] ^ ns[(i, (j + 1) % Ly)][2]\n",
    "        # ns[i][1]^ns[(i+1)%L][0]\n",
    "    nsv = [v for k, v in ns.items()]\n",
    "    m = tc.contractor(nsv, output_edge_order=[n[4] for n in nsv])\n",
    "    s = K.reshape(m.tensor, [-1])\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_fidelity(us, gs):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    s = K.cast(s, tc.dtypestr)\n",
    "    return -K.abs(tf.tensordot(s, gs, 1)) ** 2\n",
    "\n",
    "\n",
    "vgf_fidelity = K.jit(K.value_and_grad(get_fidelity))\n",
    "vgf_e = K.jit(K.value_and_grad(get_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training toward gs with history\n",
    "\n",
    "\n",
    "def get_gs_training(chi=4, steps=300, which=\"e\"):\n",
    "    target = u[:, 0]\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(8e-3))\n",
    "    us = np.random.normal(scale=0.1, size=[Lx, Ly, chi, chi, chi, chi, 2])\n",
    "    hist = []\n",
    "    ovs = []\n",
    "    for i in range(steps):\n",
    "        if which == \"e\":\n",
    "            v, gs = vgf_e(us)\n",
    "        elif which == \"f\":\n",
    "            v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        if i % 5 == 0:\n",
    "            hist.append(v)\n",
    "            state = get_state(us)\n",
    "            state = state / K.norm(state)\n",
    "            overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "            ovs.append(overlap)\n",
    "    return hist, ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gs_training(chi=4, steps=600, which=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ites(beta, chi=4, steps=300):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(8e-3))  # 4e-3 8e-3 for >1\n",
    "    us = np.random.normal(scale=0.1, size=[Lx, Ly, chi, chi, chi, chi, 2])\n",
    "    for i in range(steps):\n",
    "        v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "    state = get_state(us)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (v, slope, intercept, r, p, slopee, overlap0, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(us):\n",
    "    # mps Lx, Ly, chil, chir, chiu, chid, 2\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    c = tc.Circuit(Lx * Ly, inputs=s)\n",
    "    energy = tc.templates.measurements.operator_expectation(c, h)\n",
    "\n",
    "    return K.real(energy)\n",
    "\n",
    "\n",
    "coord = tc.templates.graphs.Grid2DCoord(Lx, Ly)\n",
    "\n",
    "\n",
    "def get_state(us):\n",
    "    depth = len(us)\n",
    "    c = tc.Circuit(12)\n",
    "    for i in range(0, Lx * Ly - 1, 2):\n",
    "        c.x(i)\n",
    "        c.h(i)\n",
    "        c.cx(i, i + 1)\n",
    "        c.x(i + 1)\n",
    "    for i in range(depth):\n",
    "        c = tc.templates.blocks.Grid2D_entangling(\n",
    "            c, coord, tc.gates._swap_matrix, us[i, 0]\n",
    "        )\n",
    "        for j in range(Lx * Ly):\n",
    "            c.ry(j, theta=us[i, 1, j])\n",
    "            c.rz(j, theta=us[i, 2, j])\n",
    "    s = c.state()\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_fidelity(us, gs):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    return -K.abs(tf.tensordot(s, gs, 1)) ** 2\n",
    "\n",
    "\n",
    "vgf_fidelity = K.jit(K.value_and_grad(get_fidelity))\n",
    "vgf_e = K.jit(K.value_and_grad(get_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ites(beta, depth=3, steps=300):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    lst = tf.keras.optimizers.schedules.ExponentialDecay(1e-2, 2000, 0.5)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(lst))\n",
    "    us = np.random.normal(scale=0.01, size=[depth, 3, Lx * Ly * 2])\n",
    "    us = K.cast(us, tc.rdtypestr)\n",
    "    for i in range(steps):\n",
    "        v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        print(v)\n",
    "    state = get_state(us)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (v, slope, intercept, r, p, slopee, overlap0, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites(1.3, depth=6, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training toward gs with history\n",
    "\n",
    "\n",
    "def get_gs_training(depth=3, steps=300, which=\"e\"):\n",
    "    target = u[:, 0]\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    lst = tf.keras.optimizers.schedules.ExponentialDecay(1e-2, 2000, 0.5)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(lst))\n",
    "    us = np.random.normal(scale=0.01, size=[depth, 3, Lx * Ly * 2])\n",
    "    us = K.cast(us, tc.rdtypestr)\n",
    "    hist = []\n",
    "    ovs = []\n",
    "    for i in range(steps):\n",
    "        if which == \"e\":\n",
    "            v, gs = vgf_e(us)\n",
    "        elif which == \"f\":\n",
    "            v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        if i % 5 == 0:\n",
    "            hist.append(v)\n",
    "            state = get_state(us)\n",
    "            state = state / K.norm(state)\n",
    "            overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "            ovs.append(overlap)\n",
    "    return hist, ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gs_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_energy(us):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    c = tc.Circuit(Lx * Ly, inputs=s)\n",
    "    energy = tc.templates.measurements.operator_expectation(c, h)\n",
    "    return K.real(energy)\n",
    "\n",
    "\n",
    "def get_state(us):\n",
    "    us = K.cast(us, tc.dtypestr)\n",
    "    s = us[: 2 ** (Lx * Ly)] + 1.0j * us[2 ** (Lx * Ly) :]\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_fidelity(us, gs):\n",
    "    state = get_state(us)\n",
    "    s = state / K.norm(state)\n",
    "    return -K.abs(tf.tensordot(s, gs, 1)) ** 2\n",
    "\n",
    "\n",
    "vgf_fidelity = K.jit(K.value_and_grad(get_fidelity))\n",
    "vgf_e = K.jit(K.value_and_grad(get_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ites(beta, steps=300):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(2e-3))\n",
    "    us = np.random.normal(scale=0.1, size=[2 * 2 ** (Lx * Ly)])\n",
    "    us = K.cast(us, tc.rdtypestr)\n",
    "    for i in range(steps):\n",
    "        v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "    state = get_state(us)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (v, slope, intercept, r, p, slopee, overlap0, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites(0.7, steps=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training toward gs with history\n",
    "\n",
    "\n",
    "def get_gs_training(steps=300, which=\"e\"):\n",
    "    target = u[:, 0]\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "    opt = K.optimizer(tf.keras.optimizers.Adam(2e-3))\n",
    "    us = np.random.normal(scale=0.1, size=[2 * 2 ** (Lx * Ly)])\n",
    "    us = K.cast(us, tc.rdtypestr)\n",
    "    hist = []\n",
    "    ovs = []\n",
    "    for i in range(steps):\n",
    "        if which == \"e\":\n",
    "            v, gs = vgf_e(us)\n",
    "        elif which == \"f\":\n",
    "            v, gs = vgf_fidelity(us, target)\n",
    "        us = opt.update(gs, us)\n",
    "        if i % 5 == 0:\n",
    "            hist.append(v)\n",
    "            state = get_state(us)\n",
    "            state = state / K.norm(state)\n",
    "            overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "            ovs.append(overlap)\n",
    "    return hist, ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gs_training(steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resblock(m, n, x):\n",
    "    y = tf.keras.layers.Dense(m, activation=\"relu\")(x)\n",
    "    y = tf.keras.layers.Dense(n, activation=\"relu\")(y)\n",
    "    return y + x\n",
    "\n",
    "\n",
    "def create_complex_model(n, w=32):\n",
    "    width = w * n\n",
    "    inputs = tf.keras.layers.Input(shape=[n])\n",
    "    lnr = resblock(width, n, inputs)\n",
    "    lnr = resblock(width, n, inputs)\n",
    "    lnr = resblock(width, n, inputs)\n",
    "    lnr = resblock(width, n, inputs)\n",
    "\n",
    "    phi = resblock(width, n, inputs)\n",
    "    phi = resblock(width, n, inputs)\n",
    "    phi = resblock(width, n, inputs)\n",
    "    phi = resblock(width, n, inputs)\n",
    "\n",
    "    lnr = 2 * tf.math.cosh(lnr)\n",
    "    lnr = tf.keras.layers.Dense(1, activation=None)(lnr)\n",
    "\n",
    "    phi = tf.keras.layers.Dense(1, activation=None)(phi)\n",
    "\n",
    "    dtype = tf.complex128\n",
    "    lnr = tf.cast(lnr, dtype=dtype)\n",
    "    phi = tf.cast(phi, dtype=dtype)\n",
    "    phi = 3.14159265j * phi\n",
    "    outputs = lnr * tf.math.exp(phi)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "base = tf.constant(list(product(*[[0, 1] for _ in range(Lx * Ly)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ites(beta, w=32, steps=500):\n",
    "    s = K.sum(\n",
    "        [K.exp(-beta / 2 * es[i]) * u[:, i] for i in range(2 ** (Lx * Ly))], axis=0\n",
    "    )\n",
    "    target = s\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "\n",
    "    class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, initial_learning_rate):\n",
    "            self.initial_learning_rate = initial_learning_rate\n",
    "\n",
    "        def __call__(self, step):\n",
    "            if step < 800:\n",
    "                return self.initial_learning_rate * 0.5 ** (step / 200)\n",
    "            return self.initial_learning_rate * 0.06\n",
    "\n",
    "    lst = LRSchedule(1e-3)\n",
    "    opt = tf.keras.optimizers.Adam(lst)\n",
    "    model = create_complex_model(Lx * Ly, w)\n",
    "\n",
    "    @tf.function\n",
    "    def value_and_grad(gs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            state = model(base)\n",
    "            state /= K.norm(state)\n",
    "            state = K.reshape(state, [-1])\n",
    "            fidelity = -K.abs(tf.tensordot(state, gs, 1)) ** 2\n",
    "        grad = tape.gradient(fidelity, model.variables)\n",
    "        return fidelity, grad\n",
    "\n",
    "    for i in range(steps):\n",
    "        v, gs = value_and_grad(target)\n",
    "        opt.apply_gradients(zip(gs, model.variables))  # 1.e-5\n",
    "    state = model(base)\n",
    "    state = state / K.norm(state)\n",
    "    overlap0 = np.abs(K.numpy(target).reshape([1, -1]) @ u) ** 2\n",
    "    overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "    slope, intercept, r, p, slopee = scipy.stats.linregress(es, np.log(overlap))\n",
    "    return (v, slope, intercept, r, p, slopee, overlap0, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ites(0.5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training toward gs with history\n",
    "\n",
    "\n",
    "def get_gs_training(w=32, steps=300, which=\"e\"):\n",
    "    target = u[:, 0]\n",
    "    target = K.reshape(target, [-1])\n",
    "    target /= K.norm(target)\n",
    "    target = K.cast(target, tc.dtypestr)\n",
    "\n",
    "    class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, initial_learning_rate):\n",
    "            self.initial_learning_rate = initial_learning_rate\n",
    "\n",
    "        def __call__(self, step):\n",
    "            if step < 800:\n",
    "                return self.initial_learning_rate * 0.5 ** (step / 200)\n",
    "            return self.initial_learning_rate * 0.06\n",
    "\n",
    "    lst = LRSchedule(1e-3)\n",
    "    opt = tf.keras.optimizers.Adam(lst)\n",
    "    model = create_complex_model(Lx * Ly, w)\n",
    "\n",
    "    if which == \"f\":\n",
    "\n",
    "        @tf.function\n",
    "        def value_and_grad():\n",
    "            with tf.GradientTape() as tape:\n",
    "                state = model(base)\n",
    "                state /= K.norm(state)\n",
    "                state = K.reshape(state, [-1])\n",
    "                fidelity = (\n",
    "                    -K.abs(tf.tensordot(state, K.cast(u[:, 0], tc.dtypestr), 1)) ** 2\n",
    "                )\n",
    "            grad = tape.gradient(fidelity, model.variables)\n",
    "            return fidelity, grad\n",
    "\n",
    "    elif which == \"e\":\n",
    "\n",
    "        @tf.function\n",
    "        def value_and_grad():\n",
    "            with tf.GradientTape() as tape:\n",
    "                state = model(base)\n",
    "                state /= K.norm(state)\n",
    "                state = K.reshape(state, [-1])\n",
    "                c = tc.Circuit(Lx * Ly, inputs=state)\n",
    "                energy = tc.templates.measurements.operator_expectation(c, h)\n",
    "                energy = K.real(energy)\n",
    "            grad = tape.gradient(energy, model.variables)\n",
    "            return energy, grad\n",
    "\n",
    "    hist = []\n",
    "    ovs = []\n",
    "    for i in range(steps):\n",
    "        v, gs = value_and_grad()\n",
    "        opt.apply_gradients(zip(gs, model.variables))  # 1.e-5\n",
    "        if i % 5 == 0:\n",
    "            hist.append(v)\n",
    "            state = model(base)\n",
    "            state = state / K.norm(state)\n",
    "            overlap = np.abs(K.numpy(state).reshape([1, -1]) @ u) ** 2\n",
    "            ovs.append(overlap)\n",
    "    return hist, ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gs_training(which=\"f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
